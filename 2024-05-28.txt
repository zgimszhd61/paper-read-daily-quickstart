论文地址：https://arxiv.org/pdf/2405.17044
ERROR:https://arxiv.org/pdf/2405.17044
论文地址：https://arxiv.org/pdf/2405.17372
模拟交通智能体之间的真实互动对于有效验证自动驾驶系统的安全性至关重要。

现有领先的模拟器主要使用编码器-解码器结构来编码历史轨迹以进行未来模拟。

然而，这种范式使模型架构复杂化，手动分离历史和未来轨迹导致数据利用率低。

为了解决这些挑战，我们提出了行为生成预训练 Transformer（Behaviorgpt），这是一个仅解码器、自回归架构，旨在模拟多个智能体的顺序运动。

关键是，我们的方法放弃了传统的“历史”和“未来”之间的分离，将每个时间步视为“当前”时间步，从而设计出更简单、更参数和数据高效的设计，能够与数据和计算无缝扩展。

此外，我们引入了下一补丁预测范式（NP3），使模型能够在轨迹的补丁级别进行推理，并捕捉长程时空交互。

Behaviorgpt 在 Waymo Sim Agents 基准测试中多个指标上排名第一，展现出在多智能体和智能体-地图互动方面的卓越性能。

我们以 0.741 的真实性得分超越了最先进模型，并将 MinADE 指标提高至 1.540，模型参数减少约 91.6%。

关键词：多智能体系统、Transformer、仅解码器架构、自动驾驶。


论文地址：https://arxiv.org/pdf/2405.17345
ERROR:https://arxiv.org/pdf/2405.17345
论文地址：https://arxiv.org/pdf/2405.16929
知识图谱（Knowledge Graphs，KGs）是公司的重要资产，因为它们在数据表示方面具有很大的灵活性，并且有许多应用，例如词汇共享、问答或推荐系统。

构建知识图谱通常依赖于从各种异构来源提取知识的自动方法。

但在一个嘈杂和不确定的世界中，知识可能不可靠，数据来源之间可能存在冲突。

整合不可靠数据将直接影响知识图谱的使用，因此必须解决这些冲突。

这可以通过手动选择最佳数据进行整合来实现。

这种第一种方法非常准确，但成本高且耗时。

因此，最近的努力集中在自动方法上，这是一项具有挑战性的任务，因为它需要处理从提取的知识到整合到知识图谱中的不确定性。

我们调查了这个方向上的最新方法，并介绍了开放和企业知识图谱的构建以及如何保持其质量。

然后我们描述了不同的知识提取方法，引入了额外的不确定性。

我们还讨论了知识获取后的下游任务，包括使用嵌入模型进行知识完善、知识对齐和知识融合，以解决知识图谱构建中的知识不确定性问题。

最后，我们讨论了在考虑不确定性时构建知识图谱时剩余的挑战和展望。

关键词：知识协调、不确定性、异构来源、知识图谱构建
论文地址：https://arxiv.org/pdf/2405.16887
WARNING:PyPDF2.generic._data_structures:Multiple definitions in dictionary at byte 0x2b1f2 for key /Rotate
随着生产力的提高，客户对多品种和小批量生产的需求不断增加，从而对制造系统提出了更高的要求。

当生产任务频繁变化以满足这种需求时，传统制造系统通常无法及时响应。

多智能体制造系统被提出来解决这一问题。

然而，由于技术限制，这种系统中智能体之间的协商是通过预定义的启发式规则实现的，这种方式并不足以处理多品种和小批量生产。

因此，本研究提出了基于大语言模型（LLM-based）的智能车间多智能体制造系统。

该系统描述了不同智能体并定义了它们的协作方法。

智能体的角色包括机器服务器智能体（MSA）、竞标邀请智能体（BIA）、竞标智能体（BA）、思考智能体（TA）和决策智能体（DA）。

由于LLM的支持，TA和DA获得了分析车间状况并选择最适合的机器的能力，而不是人为执行预定义程序。

BA和BIA之间的协商是连接制造资源的最关键步骤。

在TA和DA的支持下，BIA将根据BA返回的每台机器的信息最终确定订单分配。

MSA负责将智能体与物理车间连接起来。

该系统旨在通过智能体的协作和不同角色的分工来分配和传递工件，与其他调度方法有所区别。

还进行了比较实验以验证该系统的性能。

关键词：大语言模型（LLM）、多智能体、制造系统、智能车间。


论文地址：https://arxiv.org/pdf/2405.16604
ERROR:https://arxiv.org/pdf/2405.16604
论文地址：https://arxiv.org/pdf/2405.16640
转载需注明出处。

未经许可不得复制、转载到服务器或分发至列表，须事先获得特定许可和/或支付费用。

请向permissions@acm.org 请求权限。

©2024 计算机协会。

0004-5411/2024/5-art $15.00 https://doi.org/xxxxxxx.xxxxxxx j. acm, vol
论文地址：https://arxiv.org/pdf/2405.16751
我们致力于解决多智能体合作的挑战，其中智能体通过与 3D 场景互动，并在复杂的部分可观察性条件下与分散的智能体合作，实现共同目标。

这涉及管理通信成本，并优化动态环境中的交互轨迹。

我们的研究集中在现有合作智能体系统的三个主要局限性上。

首先，当前系统在通过观察获取信息的管理方面表现出低效，导致随着环境变得更加复杂，增加物体或目标后，规划性能下降。

其次，在部分可观察设置中忽略错误计划会导致合作性能次优，因为智能体难以适应受其他智能体未见行动影响的环境变化。

最后，未将空间数据纳入决策过程会限制智能体构建优化轨迹的能力。

为了克服这些限制，我们提出了增强相关性和验证的合作语言智能体 (REVECA)，这是一个由 GPT-3.5 驱动的新型认知架构。

REVECA 利用相关性评估、计划验证和空间信息来增强智能体在动态和部分可观察环境中的合作效率和鲁棒性，同时最小化持续通信成本，并有效管理无关的虚拟对象。

我们广泛的实验表明 REVECA 相对于以 GPT-4.0 为驱动的先前方法具有优越性。

此外，用户研究突显了 REVECA 在实现可信赖的人工智能合作方面的潜力。

我们期望 REVECA 在游戏、XR 应用、教育工具和人形机器人等领域具有重要应用，为经济、商业和学术进步做出重大贡献。

关键词：多智能体规划 · 合作智能体 · 动态环境 · 大语言模型 1
论文地址：https://arxiv.org/pdf/2405.16693
大多数决策模型，包括成对比较方法，在假设决策者诚实的基础上进行。

然而，很容易想象到一种情况，即决策者试图操纵排名结果。

本文提出了成对比较方法中的三种简单操纵方法。

我们尝试利用适当构建的神经网络来检测这些方法。

实验结果伴随着对生成数据提出的解决方案，显示出相当高的操纵检测水平。

关键词：成对比较、操纵、神经网络、机器学习、AHP。


论文地址：https://arxiv.org/pdf/2405.16595
在具有大状态空间的决策领域中，一个主要挑战是有效选择最大化效用的行动。

近年来，诸如强化学习 (Reinforcement Learning, RL) 和搜索算法等方法已成功解决了这一问题，尽管它们存在一些差异。

强化学习定义了一个学习框架，让智能体进行探索和互动。

而搜索算法则提供了一种搜索解决方案的形式化方法。

然而，要以实际方式评估这些方法的表现通常是困难的。

受到这一问题的启发，我们专注于一个游戏领域，即四子棋 (Connect-4)，并开发了一个新颖的演化框架来评估三类算法：强化学习、极小化极大算法 (Minimax) 和蒙特卡洛树搜索 (Monte Carlo Tree Search, MCTS)。

本文的贡献有三点：i) 我们实现了这些算法的高级版本，并与它们的标准对应物进行了系统比较，ii) 我们开发了一个新颖的评估框架，称为演化锦标赛，iii) 我们对每种算法的相对性能进行了广泛评估，以比较我们的研究结果。

我们评估了不同的指标，并展示了在获胜率方面，MCTS 获得了最佳结果，而极小化极大算法和 Q 学习分别排名第二和第三，尽管后者被证明是做出决策最快的。


论文地址：https://arxiv.org/pdf/2405.16510
神经语言模型的快速发展引发了智能智能体研究的新浪潮。

与传统智能体不同，基于大语言模型的智能体（LLM 智能体）由于其出色的推理和泛化能力，已成为实现人工通用智能（AGI）的一种有前途的范式。

有效的规划对于LLM智能体在现实任务中取得成功至关重要，因此成为社区中备受追捧的主题。

当前的规划方法通常将任务转化为可执行的动作序列。

然而，在细粒度上确定复杂任务的可行或最佳序列，通常需要组合长链异构动作，仍然具有挑战性。

本文介绍了元任务规划（MTP），这是一种零样本方法，用于协作LLM为基础的多智能体系统，通过将复杂任务分解为一系列从属任务或元任务的层次结构，简化了任务规划。

然后将每个元任务映射为可执行动作。

MTP在两个严格的基准测试中进行了评估，分别是travelplanner和api-bank。

值得注意的是，MTP在travelplanner上取得了平均∼40%的成功率，远高于最先进的基线（2.92%），并且在api-bank上的表现也优于LLMAPI-42with React约14%，显示了将LLM与多智能体系统集成的巨大潜力。


论文地址：https://arxiv.org/pdf/2405.16390
在涉及安全关键系统的众多强化学习（RL）问题中，一个关键挑战在于在同时满足所有严格的安全约束的情况下平衡多个目标。

为了解决这个问题，我们提出了一个基于原始的框架，用于协调多目标学习和约束遵从之间的策略优化。

我们的方法采用了一种新颖的自然策略梯度操纵方法，以优化多个RL目标，并克服不同任务之间冲突梯度的问题，因为简单的加权平均梯度方向可能对特定任务的性能不利，这是由于不同任务目标的梯度不一致。

当硬约束条件被违反时，我们的算法会介入调整策略以最小化这种违反。

我们在表格设置中建立了理论收敛性和约束违反保证。

在实证方面，我们提出的方法在具有挑战性的安全多目标强化学习任务上也优于先前的最先进方法。

关键词：约束强化学习；多目标强化学习；梯度操纵 ∗方程贡献。

本手稿正在积极开发中。

我们欢迎任何与 shangding.gu@tum.de 相关的建设性评论和建议。

arxiv:2405.16390v1 [cs.ai] 2024年5月26日
论文地址：https://arxiv.org/pdf/2405.16334
在这项工作中，我们引入了一种新颖的方法，为大语言模型智能体（LLM agents）增加了内省能力，提升了解决复杂任务的一致性和适应性。

我们的方法促使LLM智能体将给定任务分解为可管理的子任务（即制定计划），并持续内省其行动的适宜性和结果。

我们实施了三重内省干预：1）在执行行动之前对潜在失败和替代补救措施进行预期性反思，2）在行动后与子任务目标进行对齐并进行回溯补救，以确保在计划执行中付出最大努力，3）在计划完成后进行全面审查，以便未来策略的完善。

通过在Web环境中部署和实验这种方法——一种零样本方法——用于Web环境中的实际任务，我们的智能体表现出卓越的性能，成功率为23.5%，比现有的零样本方法高出3.5%。

实验结果表明，我们基于内省的方法不仅增强了智能体通过计划执行的强大机制来应对意外挑战的能力，还通过减少完成任务所需的试验次数和计划修订次数45%，提高了效率。


论文地址：https://arxiv.org/pdf/2405.16247
大语言模型（LLM）为基础的智能体在自主完成各领域任务方面表现出了潜力，例如机器人技术、游戏和网络导航。

然而，这些智能体通常需要精心设计和专家提示才能解决特定领域的任务，这限制了它们的适应性。

我们引入了 Automanual，这是一个框架，使LLM智能体能够通过互动自主建立对环境的理解，并适应新环境。

Automanual将环境知识分类为不同规则，并通过两个智能体在线优化这些规则：1）规划者根据当前规则为与环境互动制定可行计划。

2）构建者通过一个结构良好的规则系统更新规则，促进在线规则管理和重要细节保留。

为了减轻规则管理中的幻觉，我们为构建者引入了基于案例条件的提示策略。

最后，形成器智能体将这些规则编制成全面手册。

这个自动生成的手册不仅可以提高适应性，还可以指导更小LLM的规划，同时易于人类阅读。

仅仅给出一个简单示范，Automanual显著提高了任务成功率，在 Alfworld 基准任务中，GPT-4-Turbo 达到了97.4%，GPT-3.5-Turbo 达到了86.2%。

源代码即将发布。


论文地址：https://arxiv.org/pdf/2405.16205
基因集知识发现对推动人类功能基因组学至关重要。

最近的研究表明，利用大语言模型（LLMs）在这一任务上表现出了令人期待的性能。

然而，它们的结果受到LLMs常见的一些限制，比如幻觉。

为此，我们提出了GeneAgent，这是一种首创的语言智能体，具有自我验证能力。

它能够自主与各种生物数据库进行交互，并利用相关领域知识来提高准确性并减少幻觉发生。

在来自不同来源的1,106个基因集上进行基准测试，GeneAgent始终以显著优势胜过标准的GPT-4。

此外，详细的手动审查证实了自我验证模块在减少幻觉并生成更可靠的分析叙事方面的有效性。

为了展示其实际效用，我们将GeneAgent应用于从小鼠B2905黑色素瘤细胞系中提取的七个新基因集，专家评估显示GeneAgent提供了对基因功能的新见解，并随后加快了知识发现的过程。


论文地址：https://arxiv.org/pdf/2405.16128
机器学习模型学习到的表示与人类的表示有多相符呢？在这里，我们考虑深度学习模型学习到的概念表示，并评估它们是否展现出人类概念的一个基本行为特征，即典型性效应。

这一发现是人们判断某些实例（例如知更鸟）在一个类别（例如鸟类）中比其他实例（例如企鹅）更典型的现象。

最近的研究寻找语言和视觉模型中类似人类典型性效应的研究，集中在单一模态模型上，仅测试了少量概念，并发现与人类典型性评分仅有适度相关性。

本研究通过考虑更广泛的语言（n=8）和视觉（n=10）模型架构，扩展了对模型的行为评估。

它还评估了视觉+语言模型对典型性预测的结合，以及多模态 clipbased 模型，是否比单一模态模型更好地与人类典型性判断相一致。

最后，它评估了比以往研究更广泛的概念（n=27）。

有三个重要发现。

首先，语言模型比视觉模型更符合人类典型性判断。

其次，结合语言和视觉模型（例如 alexnet + minilm）比最佳表现的语言模型（即 minilm）或视觉模型（即 vit-huge）单独更好地预测人类典型性数据。

第三，多模态模型（即 clip vit）显示出解释人类典型性判断的潜力。

这些结果推动了机器学习模型和人类概念表示相一致性的最新进展。

方法学上的贡献是为测试视觉模型的概念对齐性创造了一个新的图像集。

关键词：概念；分类；典型性效应；机器学习；多模态模型；计算建模
论文地址：https://arxiv.org/pdf/2405.16072
在本文中，我们介绍了 SynthAI，这是一种用于自动创建高级综合（HLS）设计的开创性方法。

SynthAI 将 React 智能体、思维链（COT）提示、网络搜索技术以及检索增强生成（RAG）框架整合到一个结构化决策图中。

这种创新方法使得复杂硬件设计任务能够系统化地分解为多个阶段和更小、可管理的模块。

因此，SynthAI 能够生成符合用户指定设计目标和功能要求的可综合设计。

我们通过几个案例研究进一步验证了 SynthAI 的能力，突出了它在从单个初始提示生成复杂的多模块逻辑设计方面的熟练性。

SynthAI 代码可通过以下存储库获取：https://github.com/sarashs/fpga-agi

## 一、引言

多项研究已经对大型语言模型（LLMs）在自动硬件代码生成领域的能力和局限性进行了基准测试和评估。

我们首先提供了该领域的概述。

LLMs 已经显示出在生成基本逻辑硬件设计方面具有很好的能力。

Pearce 等人[1]探讨了从自然语言中导出硬件描述代码的概念。

Blocklove 等人[2]和 Chang 等人[3]在对话式硬件设计领域讨论了进一步的进展，强调了挑战和机遇。

这三篇论文都展示了LLMs生成基本逻辑代码的能力。

在专门用于代码生成的LLMs领域，Thakur 等人[4]研究了LLMs在微调后生成 Verilog 代码的能力。

他们证明了通过使用 Verilog 数据集微调LLMs可以增强其生成符合语法的代码的能力。

他们的研究在开发评估框架方面具有开创性，用于对生成的代码进行功能分析和语法测试。

他们的工作的主要贡献在于提供开源训练、评估脚本和LLM检查点。

然而，值得注意的是，他们的方法在处理高级 Verilog 代码生成任务方面存在困难。

Fu 等人开发的LLM4Sechw框架[5]专注于利用LLMs进行硬件设计调试，特别是解决领域特定数据稀缺的问题。

他们的重要贡献在于创建了一个用于硬件设计缺陷的数据集，为硬件安全领域的新应用铺平了道路。

Liu 等人[6]介绍了 VerilogEval，这是一个旨在评估LLMs在 Verilog 代码生成中的框架。

他们的工作之所以独特，是因为建立了第一个能够在各种 Verilog 代码生成任务中进行全面评估的基准框架。

Du 等人[7]探讨了LLMs在无线信号处理中生成 HDL 代码的应用，结合了上下文学习和思维链提示。

他们的工作因为在为先进应用生成复杂的HDL代码方面具有潜力而脱颖而出，为无线通信硬件提供了有见地的贡献。



在我们的工作中，我们介绍了 SynthAI（图 3），这是一个通过使其能够利用诸如网络搜索和数据库查询等工具来增强大型语言模型（LLMs）的框架。

该框架使用多样的提示技术，如思维链（COT）、React 和上下文学习，能够在无需重新训练的情况下生成可综合和优化的代码。

SynthAI 可以访问实时数据和包含代码样本和 FPGA 数据表的全面数据库，从而在规划和执行模块化设计时能够高效地从单个提示开始。

SynthAI 代码可通过以下存储库获取：https://github.com/sarashs/fpga-agi

## 二、SynthAI 中使用的基于LLM的技术

以下我们将重点介绍我们提出的框架所使用的不同现有技术作为基础。

  

### a. MRKL 模块化推理、知识和语言（MRKL）

MRKL 是人工智能领域的一种专门系统，旨在增强大型语言模型（LLMs）的能力。

MRKL 通过增强LLMs的自然语言处理能力，使其不仅能够根据其训练数据生成响应，还能够访问和整合来自外部数据库或知识库的额外信息。

这种整合允许在LLM可能缺乏足够数据的情况下，提供更加明智和准确的响应。


论文地址：https://arxiv.org/pdf/2405.15908
自动化渗透测试（AutoPT）基于强化学习（RL）已被证明能够提高信息系统中漏洞识别的效率。

然而，基于RL的渗透测试遇到了几个挑战，包括采样效率低、奖励规范复杂以及可解释性有限。

为了解决这些问题，我们提出了一种基于知识的AutoPT框架，称为DRLRM-PT，它利用奖励机制（RMs）将领域知识编码为训练PT策略的指导方针。

在我们的研究中，我们特别关注横向移动作为一个PT案例研究，并将其制定为由RMs指导的部分可观察马尔可夫决策过程（POMDP）。

我们基于MITRE ATT&CK知识库为横向移动设计了两个RMs。

为了解决POMDP并优化PT策略，我们采用了带有RM的深度Q学习算法（DQRM）。

实验结果表明，与没有知识嵌入的智能体相比，DQRM智能体在PT中表现出更高的训练效率。

此外，编码更详细领域知识的RMs相比于具有更简单知识的RMs表现出更好的PT性能。

关键词：渗透测试 · 强化学习 · 人类知识整合 · 奖励机制
论文地址：https://arxiv.org/pdf/2405.15832
预测性维护和网络安全的整合代表了工业4.0范式下中小企业（SMEs）的一项变革性进展。

尽管在经济上具有重要意义，但由于资源限制和知识缺口，SMEs在采用先进技术方面常常面临重大挑战。

Detecta 2.0项目通过开发一种创新系统来解决这些障碍，该系统将实时异常检测、复杂分析和预测预测能力融为一体。

该系统采用半监督方法，结合了无监督异常检测和监督学习技术。

这种方法能够更灵活、更具成本效益地开发AI检测系统，显著缩短了手动案例审查所需的时间。

系统的核心是数字孪生界面，提供直观的机器状态和检测到的异常的实时可视化。

利用尖端AI引擎，系统根据观察到的模式智能地对异常进行分类，区分技术错误和潜在的网络安全事件。

这种辨别得到了详细分析的支持，包括增强警报可靠性和减少误报的确定性水平。

预测引擎使用先进的时间序列算法如n-hits来预测未来机器利用趋势。

这种积极的方法优化了维护计划，增强了网络安全措施，并在生产过程多变的情况下最小化了计划外停机时间。

由于其模块化架构能够实现在工业设置中的无缝集成和低实施成本，Detecta 2.0为SMEs加强其预测性维护和网络安全策略提供了一种具有吸引力的解决方案。

关键词：数字孪生 · 机器学习 · 网络安全 · 预测性维护 · 工业4.0 · SME
论文地址：https://arxiv.org/pdf/2405.15821
语言模型作为智能体推动了序列决策智能体的边界，但在对环境动态的知识有限和指数级巨大的行动空间方面存在困难。

最近的研究如 GLAM 和 Twosome 手动限制了行动空间到一个受限子集，并采用强化学习来使智能体的知识与特定环境相一致。

然而，它们忽视了对于行动内 Token 的细粒度信用分配，这对于语言智能体的高效优化至关重要，并依赖于人类的先验知识来限制行动空间。

本文提出了将语言智能体优化从行动级别分解到 Token 级别，为每个行动内 Token 提供更精细的监督，并在行动空间不受限制的环境中提供可管理的优化复杂性。

从简化将所有行动展平开始，我们在理论上探讨了行动级别优化与这种天真的 Token 级别优化之间的差异。

然后，我们推导出了带有行动分解的贝尔曼备份（BAD），以整合对于行动内 Token 和行动间 Token 的信用分配，有效消除了差异。

将 BAD 实现在 PPO 算法中，我们引入了带有行动分解的策略优化（POAD）。

POAD 受益于更细粒度的信用分配过程和更低的优化复杂性，从而提高了在与交互式环境中对齐语言智能体的学习效率和泛化能力。

我们在各种测试平台上验证了 POAD，结果证实了我们方法的优势和我们理论分析的正确性。


论文地址：https://arxiv.org/pdf/2405.15815
ERROR:https://arxiv.org/pdf/2405.15815
论文地址：https://arxiv.org/pdf/2405.15804
ERROR:https://arxiv.org/pdf/2405.15804
论文地址：https://arxiv.org/pdf/2405.17346
大语言模型（LLMs）在各种任务中展现出了卓越的性能。

然而，LLMs 的性能很大程度上取决于输入提示，这导致了最近关于提示优化的许多研究。

然而，先前的研究通常需要一个数值分数来评估每个提示的质量。

不幸的是，当人类用户与一个黑匣子式的LLM进行交互时，获得这样的分数通常是不可行且不可靠的。

相反，从人类用户那里获得偏好反馈通常更容易且更可靠，即向用户展示从一对提示生成的响应，并询问用户哪个更受欢迎。

因此，在本文中，我们研究了使用人类反馈进行提示优化（POHF）的问题，即我们旨在仅使用人类偏好反馈为黑匣子式LLM优化提示。

受到对抗性多臂老虎机的启发，我们设计了一个在每次迭代中选择一对提示来查询偏好反馈的理论上合理的策略，因此引入了我们的算法自动化POHF（APOHF）。

我们将我们的APOHF算法应用于各种任务，包括优化用户说明书、文本到图像生成模型的提示优化，以及使用人类反馈进行响应优化（即使用我们的APOHF的变体进一步完善响应）。

结果表明，我们的APOHF能够在少量偏好反馈实例中高效地找到一个好的提示。

我们的代码可以在https://github.com/xqlin98/apohf 找到。


论文地址：https://arxiv.org/pdf/2405.17057
代码生成在各种任务中起着至关重要的作用，例如代码自动补全和数学推理。

先前的研究提出了许多方法来提高代码生成性能，包括整合来自编译器的反馈。

受此启发，我们提出了 ReflectionCoder，这是一种新颖的方法，通过整合编译器反馈构建反射序列，有效地提高一次性代码生成性能。

此外，我们提出了反射自蒸馏和动态蒸馏，以有效利用这些反射序列。

在三个基准测试上进行了大量实验，即 Humaneval（+）、MBPP（+）和 MultiPL-E，结果表明，使用我们方法微调的模型实现了最先进的性能。

值得注意的是，ReflectionCoderDeepSeek-Coder-33B 在 Humaneval（+）上达到了 82.9（76.8）的 Pass@1，MBPP（+）上达到了 84.1（72.0），与 GPT-3.5-Turbo 和 Claude-3-Opus 不相上下，并超过了早期的 GPT-4。

在代码领域之外，我们相信这种方法可以使关注最终结果并需要长时间推理路径的其他领域受益。

代码和数据可在 https://github.com/sensellm/reflectioncoder 获取。


论文地址：https://arxiv.org/pdf/2405.16946
生物系统和机器学习算法在完成任务时所需的样本数量上有何区别？我们比较了体外生物神经网络的学习效率与最先进的深度强化学习（RL）算法在简化的“乒乓球”游戏模拟中的表现。

我们使用了 DishBrain，这是一个系统，它将体外神经网络与高密度多电极阵列的硅内计算相结合，我们对比了这些生物系统的学习速度和表现与三种最先进的深度 RL 算法（即 DQN、A2C 和 PPO）在相同游戏环境中的学习情况。

这使得我们能够对生物神经系统和深度 RL 进行有意义的比较。

我们发现，当样本数量受到现实世界时间限制时，即使是这些非常简单的生物培养物在各种游戏性能特征上也胜过了深度 RL 算法，暗示着更高的样本效率。

最终，即使在跨多种信息输入进行测试以评估更高维数据输入的影响时，生物神经元也展示出比所有深度强化学习智能体更快的学习速度。

关键词：体外 · 神经培养物 · 深度强化学习 · 合成生物智能 · 样本效率 · 电生理学 · 生物计算 · 学习 · 智能
论文地址：https://arxiv.org/pdf/2405.16922
人类和动物终身学习。

这种持续学习对智能至关重要。

在本章中，我们探讨了复杂内部突触动力学的可塑性机制可能在神经网络中发挥的关键作用，使其具备这种能力。

通过调查理论研究，我们强调了两个对于持续学习至关重要的基本因素。

首先，突触可塑性机制必须在几个行为相关的时间尺度上维持和演化内部状态。

其次，可塑性算法必须利用内部状态智能地调节单个突触的可塑性，以促进新记忆的无缝整合，同时避免对已有记忆造成有害干扰。

我们的章节涵盖了这些原则成功应用于深度神经网络的案例，并强调了突触元可塑性在维持持续学习能力中的重要性。

最后，我们概述了进一步研究的途径，以了解大脑出色的持续学习能力，并利用类似机制来构建人工智能系统。

要点 • 持续学习、适应环境并长时间保留过去记忆的能力对智能至关重要。

 • 在本章中，我们调查了大量理论和建模工作，表明大脑需要具备两个基本要求的可塑性机制才能持续学习。

 • 首先，这些可塑性机制必须在广泛的时间尺度上演化和维持内部状态或记忆。

 • 其次，“智能”的可塑性算法可以访问这种状态，调节单个突触的可塑性，形成新记忆，同时不会对先前存储的记忆造成有害干扰。

 ∗通讯：friedemann.zenke@fmi.charxiv:2405.16922v1 [q-bio.nc] 2024年5月27日
论文地址：https://arxiv.org/pdf/2405.16899
在神经科学领域，确定研究对象是否表现出基于模型的行为的关键行为测试之一是研究其对环境局部变化的适应性。

然而，在强化学习中，最近的研究表明，现代基于模型的智能体在适应这种变化方面表现不佳。

这主要原因在于现代智能体通常被设计用于提高单一任务设置中的样本效率，因此并未考虑到其他设置中可能出现的挑战。

在局部适应设置中，一个特别重要的挑战是在局部变化后快速建立和维护一个足够准确的模型。

对于深度基于模型的智能体来说，这是具有挑战性的，因为它们的模型和重放缓冲区是缺乏分布转移处理能力的单体结构。

在这项研究中，我们展示了部分模型的概念上简单的想法可以让深度基于模型的智能体克服这一挑战，从而实现构建局部适应的基于模型的智能体。

通过通过不同模型对状态空间的不同部分建模，智能体不仅可以维护一个在整个状态空间上准确的模型，还可以在环境局部变化时快速适应它。

我们通过展示在诸如深度 Dyna-Q、Planet 和 Dreamer 等智能体中使用部分模型可以使它们有效地适应其环境中的局部变化来证明这一点。


论文地址：https://arxiv.org/pdf/2405.16718
我们提出了因果摊销主动结构学习（Causal Amortized Active Structure Learning，CAASL），这是一种能够选择适应性、实时且无需访问似然性的主动干预设计策略。

这个策略是基于 Transformer 的摊销网络，通过在设计环境的模拟器上使用强化学习进行训练，以及一个奖励函数来衡量真实因果图与从收集的数据中推断出的因果图后验之间的接近程度。

在合成数据和单细胞基因表达模拟器上，我们实证表明通过我们的策略获取的数据导致对潜在因果图的估计优于替代策略。

我们的设计策略成功地实现了在训练环境的分布上摊销干预设计，同时在测试时设计环境的分布转移上也具有良好的泛化能力。

此外，我们的策略还展现出在训练时维度高于训练期间的设计环境以及未经训练的干预类型上具有出色的零样本泛化能力。


论文地址：https://arxiv.org/pdf/2405.16661
近年来，大语言模型（LLMs）已经在人工智能的各个子领域产生了巨大影响，尤其是在自然语言理解任务上。

然而，普遍认为当代大语言模型的逻辑推理能力充其量是零碎的（即在某些问题实例上可能表现良好，但在其他情况下却失败得很惨）。

虽然传统的LLM微调方法（例如使用人类反馈的方法）在一定程度上解决了这个问题，但它们存在许多问题，包括不可靠的黑盒奖励模型、收集偏好数据的困难以及稀疏的标量奖励数值。

为了解决这些挑战，我们提出了一种新的训练/微调范式，称为通过符号反馈进行强化学习（RLSF），旨在增强LLMs的推理能力。

在RLSF设置中，正在训练/微调的LLM被视为RL智能体，而环境可以访问推理或领域知识工具（例如求解器、代数系统）。

在RLSF中，这些推理工具可以通过多项式大小的证书（例如证明）向LLMs提供反馈，以描述LLM生成的对象与某些正确性规范之间的错误。

RLSF基于训练/微调利用生成证书的符号工具，为LLMs提供了可靠的细粒度（标记级）奖励信号，从而解决了上述传统奖励模型的局限性。

通过广泛的评估，我们展示了我们基于RLSF对LLMs进行微调在两个不同应用中优于传统方法，即从自然语言伪代码到编程语言（C++）的程序合成，以及解决24点游戏。

对于前者，经过RLSF微调的LLMs在编译准确性和功能正确性方面取得了显著改进（例如，与监督微调相比，Google的CodeGemma-2b的编译准确性提高了+52.64%，功能正确性提高了+31.43%）。

此外，与ChatGPT（规模大100倍）相比，CodeGemma-2b的编译准确性提高了+34.82%，功能正确性提高了+17.01%。

同样，在24点游戏中，我们观察到Meta的LLAMA2-7b相比传统方法成功率提高了+25%，与ChatGPT（规模大25倍）相比成功率提高了+7%。

一个重要发现是，通过RLSF进行微调使相对较小的LLMs能够显著胜过数量级更大的模型（例如ChatGPT）。

预印本。

正在审阅。

arXiv:2405.16661v1 [cs.cl] 2024年5月26日。


论文地址：https://arxiv.org/pdf/2405.16655
本文提出了一个框架，用于有选择地触发对传入源代码更改的安全审查。

作为代码审查服务中的审查机器人，该框架可以在代码更改提交到源代码存储库之前，在预提交时间自动请求额外的安全审查。

由于进行此类安全代码审查会增加成本，该框架采用了一个经过训练的分类器，用于识别具有高潜在漏洞可能性的代码更改。

在线分类器利用各种类型的输入特征来分析审查模式，跟踪软件工程过程，并挖掘给定代码更改中的特定文本模式。

分类器及其特征是经过精心选择和优化的，使用了来自提交的代码更改数据以及 Android 开源项目（AOSP）中报告的漏洞。

评估结果表明，我们的漏洞预防（VP）框架在数据集中识别出约 80% 的导致漏洞的代码更改，精确率约为 98%，误报率约为 1.7%。

我们讨论了在多项目设置中部署 VP 框架的影响以及 Android 安全研究的未来方向。

本文探讨并验证了我们对代码更改粒度漏洞预测的方法，提供了一种通过在提交之前预先检测到脆弱代码更改来预防软件安全的技术。

索引词：机器学习分类、安全测试、软件工程过程、漏洞预测和漏洞预防。


论文地址：https://arxiv.org/pdf/2405.16552
现有的大语言模型（LLMs）通过单向自回归解码方法生成文本，以回应各种用户查询。

这些方法往往以简单的顺序方式考虑 Token 选择，当遇到不确定的 Token 时容易陷入次优选项，我们的工作将这种情况称为混沌点。

许多混沌点存在于LLMs生成的文本中，它们通常会显著影响随后生成的 Token 的质量，从而干扰LLMs的生成。

本文提出了自我评估解码（SED），这是一种增强模型生成的解码方法。

类似于人类决策过程，SED将推测和评估步骤整合到解码过程中，使LLMs能够做出更谨慎的决策，从而优化混沌点处的 Token 选择。

通过在不同LLMs上使用SED进行各种任务的实验结果表明了SED的有效性。


论文地址：https://arxiv.org/pdf/2405.16538
这篇论文旨在开发一种新的深度学习启发的游戏方法，用于早期痴呆症的检测。

该研究通过一个基于认知评估的游戏应用，将健康指标数据和面部图像数据整合到一个强大的基于卷积神经网络（CNN）的模型中，用于早期痴呆症检测。

我们从加尔各答阿波罗诊断中心收集了1000个健康指标数据样本，这些数据被标记为“痴呆”或“非痴呆”，用于训练游戏第一级别的 mod-1d-cnn 模型，另外，我们的研究团队还收集了包含1800个面部数据的面部图像数据集，这些数据被标记为“痴呆”或“非痴呆”，用于训练游戏第二级别的 mod-2d-cnn 模型。

在我们的工作中，提出的 mod-1d-cnn 模型的损失为0.2692，最高准确率为70.50%，用于识别使用真实生活健康指标数据的痴呆特征。

同样，提出的 mod-2d-cnn 模型损失为0.1755，在这里获得的最高准确率为95.72%，用于识别使用真实生活面部图像数据的痴呆状态。

因此，我们应用了基于规则的加权方法来结合这两种提出的方法以达到最终决策。

mod-1d-cnn 和 mod-2d-cnn 模型是更轻量级和计算效率更高的选择，因为与其他最先进的模型相比，它们的参数数量显著较低。

我们将它们的准确性和参数与其他最先进的深度学习模型进行了比较。

关键词：认知评估、痴呆检测、深度学习、卷积神经网络、游戏游玩
论文地址：https://arxiv.org/pdf/2405.16450
程序化强化学习（Programmatic Reinforcement Learning，PRL）已被探索用于通过程序表示策略，以实现可解释性和泛化能力。

尽管有着令人期待的结果，但当前最先进的PRL方法受到样本效率低下的限制，需要数千万次程序-环境交互。

为了解决这一挑战，我们引入了一种新颖的大语言模型引导搜索框架（LLM-GS）。

我们的关键洞察是利用大语言模型的编程专业知识和常识推理，以增强无假设、随机猜测搜索方法的效率。

我们通过提出一种Pythonic-DSL策略来解决大语言模型无法生成精确和语法正确的特定领域语言（DSL）程序的挑战——即指导大语言模型首先生成Python代码，然后将其转换为DSL程序。

为了进一步优化大语言模型生成的程序，我们开发了一种名为Scheduled Hill Climbing的搜索算法，旨在高效地探索程序化搜索空间，持续改进程序。

在Karel领域的实验结果展示了我们的LLM-GS框架的卓越有效性和效率。

广泛的消融研究进一步验证了我们的Pythonic-DSL策略和Scheduled Hill Climbing算法的关键作用。


论文地址：https://arxiv.org/pdf/2405.16439
模仿学习（Imitation Learning，IL）策略被用于通过学习人类轨迹生成机器人运动规划和导航策略。

最近，在应用 IL 解决城市环境中的社交互动问题，如大学校园、餐馆、杂货店和医院等方面引起了很多关注。

然而，在社交环境中获取大量专家示范可能昂贵、风险高，甚至不可能。

因此，当前方法主要集中在模拟社交互动场景上。

这引发了一个问题：机器人如何能够从真实世界的多智能体社交互动场景中学习模仿专家示范者？目前尚不清楚哪种（如果有的话）IL 方法表现良好以及它们需要什么假设。

我们在德克萨斯大学奥斯汀分校收集的新型行人路口数据集上，在真实世界社交互动场景中对代表性 IL 方法进行基准测试，评估了一个运动规划任务。

我们的评估揭示了两个关键发现：首先，学习多智能体成本函数对于学习紧密耦合互动中智能体的多样行为模式是必要的；其次，在模拟中将 IL 方法的训练条件化为部分状态信息或提供全局信息可以改善模仿学习，特别是在真实世界社交互动场景中。



I. 引言
在人口密集的人类环境中的机器人导航引起了广泛关注。

特别感兴趣的是行人、滑板车和车辆之间在几何约束场景中的紧密社交互动，如十字路口、走廊、狭窄缝隙等。

这些场景，我们称之为社交小游戏或SMGs，既发生在室外环境，也发生在室内空间，如餐馆、杂货店、医院和大学校园。

尽管取得了一些成功，在社交遵从方面部署机器人，例如让行给行人，是具有挑战性的，因为社交遵从需要预测和反应人类的意图以及展示他们自己的意图。

由于人类在以社交遵从方式导航社交小游戏方面最为擅长，模仿学习在社交机器人导航中发挥了重要作用，其中机器人导航策略通过模仿人类轨迹进行学习。

这项工作在德克萨斯大学奥斯汀分校的自主移动机器人实验室（AMRL）和学习智能体研究组（LARG）中进行。

AMRL 的研究部分得到了 NSF（CAREER-2046955，IIS-1954778，CCF-2006404，OIA-2219236，DGE-2125858，CCF-2319471）、ARO（W911NF-23-2-0004）、亚马逊和 JP 摩根的支持。

LARG 的研究部分得到了 NSF（FAIN-2019844，NRT-2125858）、ONR（N0001418-2243）、ARO（E2061621）、博世、洛克希德·马丁以及德克萨斯大学奥斯汀分校的 Good Systems Grand Challenge 的支持。

Peter Stone 担任 Sony AI America 的执行董事，并因此获得财务补偿。

这项安排的条款已经根据德克萨斯大学奥斯汀分校关于研究客观性的政策进行了审查和批准。

Negar Mehr 得到了 NSF CCF-2211542 的支持。

图
论文地址：https://arxiv.org/pdf/2405.16433
利用大语言模型（LLMs）辅助心理咨询是当前一个重要但具有挑战性的任务。

已经有人尝试改进具有同理心的对话或让LLMs在治疗中充当有效助手。

然而，现有数据集缺乏咨询知识，导致LLMs缺乏专业咨询能力。

此外，在咨询过程中如何自动评估多轮对话仍是一个研究不足的领域。

为了弥补这一差距，我们提出了CpsyCoun，一个基于报告的中文心理咨询多轮对话重构和评估框架。

为了充分利用心理咨询报告，我们设计了一个两阶段方法来构建高质量的对话，同时开发了一个全面的评估基准，用于有效自动评估多轮心理咨询。

竞争性实验结果表明我们提出的框架在心理咨询中的有效性。

我们已经开源数据集和模型供未来研究使用。


论文地址：https://arxiv.org/pdf/2405.16405
图神经网络（GNNs）在各种应用中表现出色，但仍然容易受到对抗性攻击的影响，特别是图注入攻击（GIAs），这种攻击会向原始图中注入恶意节点，构成现实威胁。

文本属性图（TAGs），其中节点与文本特征相关联，由于它们在现实世界应用中的普遍性而至关重要，并且通常用于评估这些漏洞。

然而，现有研究仅关注嵌入级别的GIAs，这些攻击注入的是节点嵌入而不是实际文本内容，限制了它们的适用性并简化了检测。

在本文中，我们首次探索了文本级别的GIA，提出了三种注入文本内容到图中的新型攻击设计。

通过理论和实证分析，我们证明了文本可解释性，在嵌入级别先前被忽视的因素，在攻击强度中发挥着至关重要的作用。

在我们调查的设计中，基于词频的文本级别GIA（WTGIA）因其在性能和可解释性之间的平衡而特别引人注目。

尽管WTGIA取得了成功，但我们发现防御者可以轻松地通过定制文本嵌入方法或基于大语言模型（LLM）的预测器来增强其防御能力。

这些见解强调了进一步研究文本级别GIAs的潜力和实际意义的必要性。


论文地址：https://arxiv.org/pdf/2405.16402
将大语言模型（LLMs）整合到医疗领域中，有潜力通过开发富有同理心的面向患者的聊天机器人，显著增强患者护理和支持。

本研究探讨了一个有趣的问题：ChatGPT 能否比通常由医生提供的回复更具同理心？为了回答这个问题，我们从梅奥诊所收集了一组患者消息和医生回复的去标识化数据集，并使用 ChatGPT 生成了替代回复。

我们的分析结合了新颖的同理心排名评估（EmRank），涉及自动化指标和人类评估，以衡量回复的同理心水平。

我们的研究结果表明，由大语言模型驱动的聊天机器人有潜力在传递同理心沟通方面超越人类医生，为增强患者护理和减少专业倦怠提供了一个有前途的途径。

该研究不仅突出了同理心在患者互动中的重要性，还提出了一套有效的自动同理心排名指标，为在医疗保健领域更广泛采用大语言模型铺平了道路。


论文地址：https://arxiv.org/pdf/2405.16363
传统的推荐系统受到强烈的反馈循环影响，通过学习和强化过去用户-物品的互动，从而限制了对新颖用户兴趣的发现。

为了解决这一问题，我们引入了一个混合层次框架，结合了大语言模型（LLMs）和经典推荐模型，用于用户兴趣探索。

该框架通过“兴趣聚类”控制LLMs和经典推荐模型之间的接口，其粒度可以由算法设计者明确确定。

它通过首先用语言表示“兴趣聚类”，然后利用经过微调的LLM生成严格位于这些预定义聚类内的新颖兴趣描述，推荐下一个新颖兴趣。

在低层次上，通过限制经典推荐模型（在本例中是基于Transformer的序列推荐器）返回落在高层次生成的新颖聚类内的物品，将这些生成的兴趣与物品级别策略联系起来。

我们展示了这种方法在为数十亿用户提供服务的工业规模商业平台上的有效性。

实时实验显示，对新颖兴趣的探索和整体用户对平台的喜爱度都显著增加。

CCS概念 •信息系统→信息检索。

*表示等同贡献。

未经费用许可，允许将本作品的全部或部分数字或纸质副本用于个人或课堂使用，前提是不得为盈利或商业优势而制作或分发副本，并且副本必须带有本通知和第一页的完整引用。

对于本作品的组成部分的版权归其他人所有的情况，必须尊重版权。


论文地址：https://arxiv.org/pdf/2405.16386
技能是有效的时间性
论文地址：https://arxiv.org/pdf/2405.16194
模仿学习旨在从观察专家演示中学习策略，而无需访问环境中的奖励信号。

生成对抗式模仿学习（Generative Adversarial Imitation Learning，GAIL）将模仿学习表述为对抗学习，利用生成器策略学习来模仿专家行为，鉴别器学习来区分专家演示和智能体轨迹。

尽管取得了令人鼓舞的结果，但 GAIL 训练通常脆弱且不稳定。

受最近扩散模型在生成建模中的主导地位启发，我们提出扩散奖励对抗模仿学习（DiffusionReward Adversarial Imitation Learning，DRAIL），将扩散模型整合到 GAIL 中，旨在为策略学习提供更稳健和更平滑的奖励。

具体而言，我们提出了扩散判别分类器来构建增强的鉴别器，并基于分类器的输出设计扩散奖励用于策略学习。

我们在导航、操作和运动领域进行了大量实验，验证了 DRAIL 相对于先前的模仿学习方法的有效性。

此外，额外的实验结果展示了 DRAIL 的泛化能力和数据效率。

GAIL 和 DRAIL 的可视化学习奖励函数表明，DRAIL 能够产生更稳健和更平滑的奖励。


论文地址：https://arxiv.org/pdf/2405.15994
可靠学习安全自主控制是值得信赖的自主性的核心问题之一。

然而，训练一个可以被正式验证为安全的控制器仍然是一个重大挑战。

我们引入了一种新颖的方法，用于在非线性神经动力系统中学习经过验证的安全控制策略，同时最大化整体性能。

我们的方法旨在通过有限时间跨越证明实现安全性，并由三个关键部分组成。

第一部分是一种新颖的课程学习方案，通过迭代增加经过验证的安全时间跨度。

第二部分利用基于梯度的学习的迭代性质，利用增量验证，重复使用先前验证运行的信息。

最后，我们学习多个经过验证的初始状态相关控制器，这个想法在更复杂的领域中尤为有价值，因为学习单个通用的经过验证的安全控制器是极具挑战性的。

我们在五个安全控制问题上的实验表明，我们训练的控制器可以在时间跨度上实现经过验证的安全性，其长度比现有技术基线长一个数量级，同时保持高奖励，以及在整个周期内完美的安全记录。


论文地址：https://arxiv.org/pdf/2405.15985
ERROR:https://arxiv.org/pdf/2405.15985
论文地址：https://arxiv.org/pdf/2405.15960
ERROR:https://arxiv.org/pdf/2405.15960
论文地址：https://arxiv.org/pdf/2405.15902
近年来，大语言模型（LLMs）在复杂性和流畅性方面取得了巨大进展，这意味着人类历史上首次可以仅通过自然语言与计算机进行交互。

这创造了自动化和计算机可访问性的巨大可能性，但也带来了严重的安全威胁：当每个人都可以与LLMs交互时，每个人都潜在地可以侵入运行LLMs的系统。

只需巧妙运用语言即可。

本文介绍了“HACC-MAN”这款游戏，挑战玩家“越狱”LLM：颠覆LLM以输出其非预期的内容。

越狱处于创造性问题解决和LLM安全之间的交汇点。

游戏的目的有三个：1. 提高人们对在日常系统中部署脆弱LLMs风险的意识，2. 提高人们与LLMs互动的自我效能感，3. 发现人们在这一新领域中采用的创造性问题解决策略。

CCS概念•安全与隐私→安全与隐私的人类和社会方面；•以人为中心的计算→交互设备。

关键词LLM安全、创造力、创造性问题解决、黑客、越狱、红队行动、街机游戏。

ACM参考格式：Matheus Valentim, Jeanette Falk, 和 Nanna Inie. 2024. HACC-MAN：一款用于越狱LLMs的街机游戏。

在ACM SIGCHI人机交互系统设计会议论文集中（预印本已被接受至DIS '24）。

ACM，美国纽约，纽约，4页。

https://doi.org/10.1145/nnnnnnn.nnnnnnn 1
论文地址：https://arxiv.org/pdf/2405.15793
软件工程是一项具有挑战性的任务，需要精通代码生成和与计算机交互的能力。

在本文中，我们介绍了 SWE-Agent，这是一个利用语言模型与计算机进行交互以解决软件工程任务的自主系统。

我们展示了一个定制的智能体-计算机接口（ACI）极大地增强了智能体创建和编辑代码文件、浏览整个代码库以及执行程序的能力。

在 SWE-Bench 上，SWE-Agent 能够解决 12.5% 的问题，而之前最佳方法检索增强生成（RAG）只能解决 3.8%。

我们探讨了 ACI 设计如何影响智能体的行为和性能，并提供了有效设计的见解。


论文地址：https://arxiv.org/pdf/2405.15786
ERROR:https://arxiv.org/pdf/2405.15786
