论文地址：https://arxiv.org/pdf/2405.15083
最近，DreamerV3 智能体在各种领域展示了最先进的性能，通过使用像素重构损失在潜在空间中学习强大的世界模型。

然而，虽然重构损失对 Dreamer 的性能至关重要，但它也需要对不必要的信息进行建模。

因此，当观察中存在视觉干扰时，Dreamer 有时会无法感知到对任务解决至关重要的元素，从而显著限制了其潜力。

在本文中，我们提出了MuDreamer，这是一个强大的强化学习智能体，它在 DreamerV3 算法的基础上构建，通过学习一个预测性世界模型，而无需重构输入信号。

与依赖像素重构不同，隐藏表示是通过预测环境价值函数和先前选择的动作来学习的。

类似于图像的预测性自监督方法，我们发现批归一化的使用对于防止学习崩溃至关重要。

我们还研究了模型后验损失和先验损失之间的 KL 平衡对收敛速度和学习稳定性的影响。

我们在常用的 DeepMind 视觉控制套件上评估了 MuDreamer，并展示了与 DreamerV3 和其他无重构方法相比对视觉干扰具有更强的鲁棒性，将环境背景替换为与任务无关的真实世界视频。

我们的方法在 Atari100k 基准上也取得了可比的性能，同时训练速度更快。


论文地址：https://arxiv.org/pdf/2405.15154
生成模型在各种任务中表现出色，使得围绕机器学习模型的交易成为可能。

本文旨在提出一种新颖的提示交易场景，即提示捆绑交易（PBT）系统，并提出一种在线定价机制。

基于组合多臂赌博机（CMAB）和三阶段分层斯塔克尔堡（HS）博弈，我们的定价机制同时考虑了消费者、平台和卖家的利润，实现了这三个参与者的利润满意度。

我们将定价问题分解为两个步骤，即未知类别选择和激励策略优化。

前者是选择一组具有最高质量的类别，后者是根据所选类别为每个参与者制定最佳策略。

与现有的固定定价模式不同，我们提出的PBT定价机制更加灵活多样，更符合真实场景的交易需求。

我们在模拟文本到图像数据集上测试了我们的方法。

实验结果证明了我们算法的有效性，为提示市场提供了可行的定价标准。


论文地址：https://arxiv.org/pdf/2405.15624
对大语言模型（LLMs）进行对齐对于增强其安全性和实用性至关重要。

然而，现有方法主要基于偏好数据集，面临着诸如标签噪声、高注释成本和隐私问题等挑战。

在这项工作中，我们引入了基于演示的对齐（AFD），这是一种利用高质量演示数据来克服这些挑战的新方法。

我们在一个顺序决策框架中形式化了AFD，突出了其独特的缺失奖励信号挑战。

借鉴前向和逆向强化学习的见解，我们为AFD引入了发散最小化目标。

在分析上，我们阐明了各种方法的质量覆盖和模式寻找行为，解释了为什么某些方法在何时以及为什么会更优越。

在实践中，我们提出了一种计算高效的算法，通过针对AFD的定制奖励模型进行外推。

我们通过对无害和有益任务的实验验证了我们的关键见解，展示了它们在保持简单性的同时表现出色的实证性能。


论文地址：https://arxiv.org/pdf/2405.15614
尽管采用了各种方法来检测漏洞，但多年来报告的漏洞数量呈上升趋势。

这表明问题在代码发布之前没有被发现，可能由许多因素引起，如缺乏意识、现有漏洞检测工具的效率有限或工具不够用户友好。

为了帮助解决传统漏洞检测工具的一些问题，我们提出使用大语言模型（LLMs）来协助发现源代码中的漏洞。

LLMs已经展现出了理解和生成代码的卓越能力，突显了它们在与代码相关任务中的潜力。

我们的目标是测试多个最先进的LLMs，并确定最佳的提示策略，从而充分利用LLMs的价值。

我们概述了基于LLM的方法的优势和劣势，并将结果与传统静态分析工具进行比较。

我们发现，LLMs可以发现比传统静态分析工具更多的问题，在召回率和F1分数方面优于传统工具。

这些结果应该有利于负责确保代码没有漏洞的软件开发人员和安全分析人员。



I. 引言
随着软件被整合到许多业务流程中，每天都会编写大量代码。

高级语言和框架通过将各种功能和安全组件引入软件，减轻了开发人员的责任。

例如，高级语言增加了许多功能和安全组件，
论文地址：https://arxiv.org/pdf/2405.15613
自监督特征是现代机器学习系统的基石。

通常在需要大量人力的数据集上进行预训练。

这种手动过程存在一些类似于监督学习中遇到的限制，例如，众包选择数据成本高昂且耗时，阻碍了数据集规模的扩展。

在这项工作中，我们考虑了自监督预训练高质量数据集的自动筛选问题。

我们认为这些数据集应该是大型、多样化且平衡的，并提出了一种基于聚类的方法来构建满足所有这些标准的数据集。

我们的方法涉及在大型多样化数据存储库上连续和分层应用 k-means，以获得在数据概念之间均匀分布的聚类，然后从这些聚类中进行分层平衡抽样。

对包括基于网络的图像、卫星图像和文本在内的三个不同数据领域进行了大量实验，结果表明，在我们自动筛选的数据集上训练的特征优于在未筛选数据上训练的特征，同时与在手动筛选数据上训练的特征相当甚至更好。


论文地址：https://arxiv.org/pdf/2405.15561
工作场所学习被用来系统地培训员工，例如通过电子学习或一对一培训。

然而，这种方法通常被认为效果不佳且成本高昂。

纯粹的电子学习缺乏对话练习和个人接触的可能性，而带有人类教练的一对一培训涉及高水平的人员和组织成本。

因此，基于生成式 AI 的教育对话智能体 (PCAs) 似乎可以弥补这两种形式的缺点。

根据行动设计研究，本文描述了一种带有生成式 PCA (GenPCA) 的组织沟通培训。

评估显示了令人鼓舞的结果：员工对该智能体的感受积极，并有助于提高自主学习。

然而，整合这种智能体并非没有局限性。

我们总结了关于教学方法的建议，这些方法得到了 GenPCA 的支持，并提出了关于改进工作场所学习中这种智能体的可能改进措施。

关键词：生成式 AI，工作场所学习，教育对话智能体，公共服务。


论文地址：https://arxiv.org/pdf/2405.15512
近年来，大语言模型（LLMs）在生成计算机代码方面取得了显著进展，模糊了人类编写的代码与人工智能（AI）生成的代码之间的界限。

随着这些技术的快速发展，探索它们如何影响代码生成至关重要，特别是考虑到在高等教育等领域可能存在的误用风险。

本文通过使用先进的分类技术来区分人类编写的代码和由ChatGPT（一种LLM类型）生成的代码，探讨了这一问题。

我们采用了一种新方法，将强大的嵌入式特征（黑盒）与监督学习算法（包括深度神经网络、随机森林和极限梯度提升）相结合，以惊人的98%准确率实现了这种区分。

对于成功的组合，我们还研究了它们的模型校准，表明其中一些模型的校准非常好。

此外，我们提出了白盒特征和可解释的贝叶斯分类器，以阐明代码来源之间的关键差异，增强了我们方法的可解释性和透明性。

这两种方法都表现良好，但最多提供85-88%的准确率。

我们还展示了未经训练的人类在解决相同任务时并不比随机猜测更好。

这项研究对于理解和减轻在代码生成中使用AI可能带来的潜在风险至关重要，特别是在高等教育、软件开发和竞技编程等背景下。

关键词：AI、机器学习、代码检测、ChatGPT、大语言模型 1.
论文地址：https://arxiv.org/pdf/2405.15324
自动驾驶在传感器、机器学习和人工智能的改进下取得了显著进展。

然而，目前的方法在复杂场景和因果关系方面存在困难，限制了在不同环境中的适应性和可解释性。

为解决上述问题，我们引入了 LeapAD，这是一种受人类认知过程启发的自动驾驶新范式。

具体来说，LeapAD 模拟人类注意力，通过选择与驾驶决策相关的关键对象，简化环境解释，并减轻决策复杂性。

此外，LeapAD 还融入了一种创新的双过程决策模块，包括用于彻底分析和推理的分析过程（系统-II），以及用于快速和经验性处理的启发式过程（系统-I）。

分析过程利用其逻辑推理来积累语言驾驶经验，然后通过监督微调将其传递给启发式过程。

通过反思机制和不断增长的记忆库，LeapAD 在封闭环境中不断从过去的错误中改进自身。

在 Carla 中的封闭环境测试表明，LeapAD 胜过所有仅依赖摄像头输入的方法，只需 1-2 个数量级更少的标记数据。

实验还表明，随着记忆库的扩大，仅具有 18 亿参数的启发式过程可以继承来自由 GPT-4 动力的分析过程的知识，并实现持续的性能改进。

代码将在 https://github.com/pjlab-adg/leapad 上发布。


论文地址：https://arxiv.org/pdf/2405.15318
学习和部署大语言模型 (Long-LLMs) 仍然是一个具有挑战性的问题，尽管最近取得了一些进展。

在这项工作中，我们认为大语言模型并不是解决长上下文任务的必要条件，因为常见的长上下文任务可以通过仅使用长上下文任务输入中的短上下文来解决，即它们可以通过与长上下文任务输入中的短上下文进行纯粹的工作来解决。

基于这一观点，我们提出了一个名为 LC-Boost (Long-Context Bootstrapper) 的框架，该框架使得短语言模型能够以引导方式解决长上下文任务。

在我们的框架中，短语言模型自身提示自己进行两个关键决策的推理：1) 如何访问输入中的适当上下文部分，2) 如何有效利用所访问的上下文。

通过根据呈现的任务自适应地访问和利用上下文，LC-Boost 可以作为处理多样化长上下文处理问题的通用框架。

我们全面评估了来自流行的长上下文基准测试中不同类型的任务，其中 LC-Boost 能够在资源消耗更少的情况下实现显着改善的性能。


论文地址：https://arxiv.org/pdf/2405.15250
聊天机器人在促进自我反思方面的作用已被广泛认可，尤其在引导用户行为改变方面。

虽然在诸如医疗保健和辅导等领域已经证明了全天候可用、可扩展性和一致性回应的好处，帮助人们养成新习惯，但它们在需要更深层内省对话以促进领导力成长的辅导中的运用尚未被探索。

本文探讨了由最新的大语言模型（LLMs）驱动的这种聊天机器人与专业教练在高管辅导领域合作的潜力。

通过与他们进行设计研讨会以及涉及十对教练和客户的两周用户研究，我们探讨了整合聊天机器人以辅助人类教练的可行性和细微差别。

我们的研究结果突出了聊天机器人的无处不在和由LLMs实现的推理能力，同时确定了它们的局限性和有效协作所需的设计要求。

通过这样做，本研究为通过人机协作方式增强个人自我反思过程奠定了基础。

CCS概念 • 人本计算 → 交互设计；协作和社交计算。

关键词：辅导、反思、聊天机器人、人工智能协作。

ACM参考格式：Riku Arakawa 和 Hiromu Yakura. 2024. 辅导副驾驶员：LLM驱动的聊天机器人与人类教练的混合形式，有效支持领导力成长的自我反思。

在ACM 2024年会话用户界面（CUI '24）中，2024年7月8-10日，卢森堡，卢森堡。

ACM，美国纽约，纽约，14页。

https://doi.org/10.1145/3640794.3665549
论文地址：https://arxiv.org/pdf/2405.15194
强化学习 (RL) 在奖励稀疏领域中存在样本效率低的问题，如果存在随机转换，问题会更加突出。

为了提高样本效率，奖励塑造是一种被广泛研究的方法，可以引入内在奖励，帮助 RL 智能体更快地收敛到最优策略。

然而，为每个问题设计一个有用的奖励塑造函数是具有挑战性的，即使对于领域专家也是如此。

他们要么必须依赖于特定任务的领域知识，要么为每个任务独立提供专家演示。

鉴于大语言模型 (LLMs) 在众多自然语言任务中迅速崭露头角，我们的目标是回答以下问题：我们能否利用 LLMs 构建一个奖励塑造函数，以提高 RL 智能体的样本效率？在这项工作中，我们旨在利用现成的 LLMs 通过解决一个更简单的确定性问题生成一个指导策略。


论文地址：https://arxiv.org/pdf/2405.15165
应用大语言模型 (LLMs) 于学术 API 使用，展现出减少研究人员学术信息检索工作的潜力。

然而，当前的LLM API使用方法在处理学术查询中常见的复杂API耦合方面存在困难。

为了解决这一问题，我们引入了 Soay，一种基于解决方案的LLM API使用方法，用于学术信息检索。

它使用带有解决方案的代码作为推理方法，其中解决方案是预先构建的API调用序列。

解决方案的添加降低了模型理解API之间复杂关系的难度。

代码提高了推理效率。

为了评估 Soay，我们引入了 SoayBench，一个评估基准，配备了 SoayEval，建立在 AMiner 的API克隆环境之上。

实验结果表明，与最先进的LLM API基线相比，性能提升了34.58-75.99%。

所有数据集、代码、调整模型和部署的在线服务都可以在 https://github.com/ruckbreasoning/soay 上公开访问。


论文地址：https://arxiv.org/pdf/2405.15143
**智能 Go-Explore（Intelligent Go-Explore）**

**go-explore** 是一组强大的算法家族，旨在解决难以探索的问题，其设计理念是存档已发现的状态，并迭代地返回并从最有前途的状态进行探索。

这种方法已经在各种具有挑战性的问题上取得了超越人类水平的表现，包括 Atari 游戏和机器人控制，但需要手动设计启发式来引导探索（即确定要保存和从中探索的状态，以及考虑下一步采取的行动），这在一般情况下是耗时且不可行的。

为了解决这个问题，我们提出了**智能 Go-Explore（Intelligent Go-Explore，简称 IGE）**，它通过用巨大的预训练基础模型（FMS）捕捉的智能和内在的人类有趣性概念，极大地扩展了原始 go-explore 的范围。

这使得 IGE 具有类似人类的能力，能本能地识别任何新状态的有趣程度或前景（例如发现新对象、位置或行为），即使在难以定义启发式的复杂环境中也能做到。

此外，IGE 提供了一个令人兴奋且以前不可能的机会，即识别并利用无法提前预测的偶然发现。

我们在需要搜索和探索的各种基于语言的任务上评估了我们的算法。

在 24 点游戏中，这是一个测试多步数学推理的问题，IGE 达到了100% 的成功率，比最佳经典图搜索基线快了 70.8%。

接下来，在 BabyAI-Text 中，这是一个具有挑战性的部分可观察网格世界，在这个世界中，代理需要遵循语言指令，IGE 以数量级更少的在线样本超越了先前的最先进技术。

最后，在 TextWorld 中，一个丰富的文本游戏中，我们展示了 IGE 在需要长期探索的环境中成功的独特能力，而先前最先进的 FM 代理（如 Reflexion）完全失败。

总的来说，智能 Go-Explore 结合了 FMS 的巨大优势和强大的 go-explore 算法，开辟了一个新的研究领域，旨在创建更具有印象深刻的探索能力的普遍能力代理。

我们所有的代码都在以下链接开源：[https://github.com/conglu1997/intelligent-go-explore](https://github.com/conglu1997/intelligent-go-explore)
论文地址：https://arxiv.org/pdf/2405.15054
在多智能体强化学习（MARL）中，行为多样性的研究是一个新兴而有前景的领域。

在这个背景下，本研究探讨了如何控制多智能体系统的多样性。

由于目前没有现有方法可以将多样性控制到一个固定值，当前的解决方案主要集中在通过内在奖励或额外损失函数盲目促进多样性，从而有效地改变学习目标，但缺乏一个原则性的衡量标准。

为了解决这个问题，我们引入了多样性控制（DICO）方法，能够通过将策略表示为参数共享组件和动态缩放的每个智能体组件的总和，将多样性控制到给定度量的精确值。

通过直接将约束应用于策略架构，DICO保持学习目标不变，使其适用于任何演员-评论家MARL算法。

我们在理论上证明了DICO实现了所需的多样性，并提供了几个合作和竞争任务的实验，展示了DICO如何作为一种新范式来提高MARL中的性能和样本效率。

有关多媒体结果，请访问论文网站。


论文地址：https://arxiv.org/pdf/2405.15019
语言条件下的机器人技能使得将大语言模型（LLMs）的高级推理应用于低级机器人控制成为可能。

一个仍然存在的挑战是获取多样化的基本技能。

现有方法要么以自顶向下的方式手动将复杂任务分解为原子级机器人动作，要么以自底向上的方式引导尽可能多的组合，以涵盖更广泛的任务可能性。

然而，这些分解或组合需要一个初始的技能库。

例如，一个仅包含多样化的“推动”技能的技能库中永远无法出现“抓取”能力。

现有的基于强化学习的技能发现技术通过详尽的探索获取技能，但往往产生无意义的行为。

在本研究中，我们介绍了一个完全由LLMs驱动的技能发现新框架。

该框架以LLM生成基于提供的场景描述和机器人配置的任务提案为起点，旨在在任务完成后逐步获取新技能。

对于每个提议的任务，启动一系列强化学习过程，利用LLM采样的奖励和成功确定函数来发展相应的策略。

通过一个独立的视觉-语言模型进一步确保学习行为的可靠性和可信度。

我们展示，从零技能开始，ASD技能库逐渐涌现并扩展到更多有意义且可靠的技能，使机器人能够高效地提出并完成高级任务。

项目页面可在以下网址找到：agentic-skill-discovery.github.io。

... 推动方块 A 和方块 B 靠近将盘子放置到目标位置将末端执行器移动到目标位置拾起盘子打开抽屉取盘子拾起方块 A...我可以学会做什么？拾起方块 BLLMs RL + 图 1: Agentic Skill Discovery 逐渐获取用于桌面操作的上下文技能。

∗通讯作者，个人网页：xf-zhao.github.io 预印本。

正在审阅。

arXiv:2405.15019v1 [cs.ro] 2024年5月23日
论文地址：https://arxiv.org/pdf/2405.15007
我们介绍了 re-adapt，这是一种在新领域对大型语言模型进行微调的方法，而不会降低任何现有的指导微调。

我们对一个适配器进行了逆向工程，该适配器隔离了指导微调模型在其对应的预训练基础模型之外学到的内容。

重要的是，这不需要额外的数据或训练。

然后，我们可以在新领域微调基础模型，并使用逆向工程的适配器重新适应指导微调。

re-adapt 和我们的低秩变体 lore-adapt 在多个流行的大语言模型和数据集上都优于其他微调方法，即使这些模型与检索增强生成结合使用时也是如此。


论文地址：https://arxiv.org/pdf/2405.14906
我们介绍 Autocoder，这是第一个在人类评估基准测试中超越了 GPT-4 Turbo（2024年4月）和 GPT-4o 的大型语言模型，pass@1 分别为 90.9% 和 90.2%。

此外，与 GPT-4 Turbo 和 GPT-4o 相比，Autocoder 提供了更多功能的代码解释器。

它的代码解释器可以安装外部包，而不仅限于内置包。

Autocoder 的训练数据是由结合了智能体交互和外部代码执行验证的系统创建的多轮对话数据集，我们将这种方法称为 AIEV-I NSTRUCT（指令调整与智能体交互和执行验证）。

与先前的大规模代码数据集生成方法相比，AIEV-I NSTRUCT 减少了对专有大型模型的依赖，并提供了经验证的代码数据集。

代码和演示视频可在 https://github.com/bin123apple/autocoder 找到。
