论文地址：https://arxiv.org/pdf/2405.14573
自主代理通过控制计算机执行人类任务可以提高人类生产力和应用可访问性。

然而，这一领域的进展将受到现实和可复制基准的推动。

我们提出了 Android World，这是一个完全运行的安卓环境，为 20 个真实世界的安卓应用中的 116 个程序化任务工作流提供奖励信号。

与现有的交互式环境不同，这些环境提供静态测试集，Android World 动态构建任务，这些任务可以以各种自然语言表达方式进行参数化，从而能够在更大规模和更真实的任务套件上进行测试。

奖励信号源自计算机的系统状态，使其能够跨任务变化持久存在，并且能够在不同应用之间进行扩展。

为了展示 Android World 的优势和运行方式，我们引入了一个新的计算机控制代理，M3A。

M3A 能够完成 Android World 中 30.6% 的任务，为未来工作留下了充足的空间。

此外，我们将一款流行的桌面网络代理适配到安卓平台上，但发现其在移动端效果不佳，这表明需要未来的研究来实现通用的跨领域代理。

最后，我们通过在代表性任务子集上测试 M3A 对一系列任务变化进行鲁棒性分析，结果显示任务参数的变化可以显著改变任务的复杂性，从而影响代理的表现，突显了在多样条件下测试代理的重要性。

Android World 和本文中的实验可在 https://github.com/google-research/android_world 获取。


论文地址：https://arxiv.org/pdf/2405.14411
动态数据驱动数字孪生 (DDDTs) 能够促进明智决策，并为底层系统提供优化平台。

通过借鉴动态数据驱动应用系统 (DDDAS) 的原则，DDDTs 能够制定用于反馈循环、模型更新和决策制定（包括自主决策）的计算模式。

然而，理解自主决策通常需要技术和领域特定知识。

本文探讨了使用大语言模型 (LLMs) 为 DDDTs 提供可解释性平台，通过利用领域特定知识库生成系统决策的自然语言解释。

文中还提供了智能农业的案例研究。

关键词：DDDAS · 可解释性 · 动态数字孪生 · LLM · 检索增强生成 · 智能农业
论文地址：https://arxiv.org/pdf/2405.14314
在实体任务中，为大语言模型（LLMs）的推理能力提供基础是具有挑战性的，这是因为物理世界的复杂性。

特别是，LLM规划多智能体协作需要智能体之间的沟通或信用分配作为反馈，以重新调整提出的计划并实现有效协调。

然而，现有方法过度依赖物理验证或自我反思，导致对LLMs的查询过多且低效。

在本文中，我们提出了一个新颖的多智能体协作框架，引入了强化优势反馈（READ），以实现计划的高效自我完善。

具体来说，我们通过批评者回归从LLM规划数据中学习顺序优势函数，然后将LLM规划者视为优化器，生成最大化优势函数的动作。

这赋予了LLM有远见，可以判断动作是否有助于完成最终任务。

我们通过将强化学习中的优势加权回归扩展到多智能体系统，提供了理论分析。

在OvercookedAI和Rocobench的困难变体上的实验表明，READ在成功率上超过了基线，并且显著减少了智能体的交互步骤和LLMs的查询轮数，展示了其在为LLMs提供基础方面的高效性。

更多结果请参见https://read-llm.github.io/
论文地址：https://arxiv.org/pdf/2405.13242
人类在制定自己的目标方面具有非凡的能力，从儿童游戏开始一直延续到成年。

尽管在目标和以目标为导向的行为方面进行了大量实证和计算工作，但模型仍远未能捕捉到日常人类目标的丰富性。

在这里，我们通过收集人类生成的游戏目标数据集，将其建模为产生奖励的程序，并通过程序合成生成类人类的新颖目标来弥合这一差距。

产生奖励的程序通过符号操作捕捉目标的丰富语义，这些操作包括组合、添加时间约束，并允许在行为轨迹上执行程序以评估进展。

为了构建一个目标的生成模型，我们学习了一个适应函数，覆盖了无限可能的目标程序集，并使用质量-多样性算法对新颖目标进行抽样。

人类评估者发现，当从被人类示例占据的程序空间分区中抽样时，模型生成的目标与人类创建的游戏无法区分。

我们还发现，我们模型内部的适应分数预测出被评为更有趣和更类人的游戏。

理解人类如何创建、表征和思考目标对于理解人类行为至关重要。

目标在心理学中无处不在，已从动机、人格和社会心理学、学习和决策等多个角度进行研究。

但目标是什么呢？Elliot & Fryer提供了一个可行但简化的定义：一个未来对象的表征，可接近或避免。

强化学习提供了另一种表述，将目标操作化为在一系列步骤中最大化累积奖励。

强化学习任务中的典型目标包括到达目标位置、在视频游戏或棋盘游戏中获胜，或将物体放置在指定位置（例如，图1a），成功可通过达到目标状态来描述。

相比之下，人们经常创造具有超越这些常见建模设置的丰富性的新颖、个性化目标。

Chu等人报道了Gareth Wild的例子，他为自己设定了一个不寻常的目标，即在特定杂货店停车场的每个停车位停车。

儿童经常在没有外部指导的情况下制定有趣且引人入胜的目标，比如创造“卡车载货卡车”或在单个塔中尽可能多地堆叠积木。

除了有趣外，这些游戏目标在学习结构化和解决任意问题方面发挥着关键作用。

事实上，有人认为自主设定和实现目标是人类智能的核心组成部分。

我们提出了一个将人类目标生成建模为合成产生奖励程序的框架。

将目标表示为符号程序具有几个优势，可以将代理的行为映射到表示成功程度的奖励分数。

首先，结构化语言有助于跨不同目标重复使用主题。

这种重复使用使得捕捉人类创造目标的广泛范围变得更加可行。

在图1e中，我们展示了一个简单的投球游戏（黑色部分）和四个不同的变体（红色、蓝色、粉色和棕色部分）的组合。


论文地址：https://arxiv.org/pdf/2405.14768
大语言模型（LLMs）需要知识更新以满足不断增长的世界事实，并纠正虚构的回应，促进终身模型编辑方法的发展。

更新后的知识存储在何处是模型编辑的一个基本问题。

在本文中，我们发现编辑长期记忆（直接模型参数）或工作记忆（通过检索非参数化神经网络激活/表示的知识）都会导致一个不可能的三角形——在终身编辑设置中，可靠性、泛化性和局部性无法同时实现。

对于长期记忆，直接编辑参数将导致与不相关的预训练知识或先前的编辑发生冲突（可靠性和局部性较差）。

对于工作记忆，基于检索的激活几乎无法使模型理解编辑并进行泛化（泛化性较差）。

因此，我们提出了一种智慧的方法来弥合记忆之间的差距。

在智慧中，我们设计了一个双参数记忆方案，包括用于预训练知识的主记忆和用于编辑知识的边缘记忆。

我们仅编辑边缘记忆中的知识，并训练一个路由器来决定在给定查询时经过哪个记忆。

对于持续编辑，我们设计了一种知识分片机制，其中不同的编辑集驻留在参数的不同子空间中，并随后合并到一个共享记忆中，避免冲突。

大量实验证明，智慧可以在趋势型LLM架构（如GPT、LLAMA和Mistral）下的终身模型编辑中的问答、虚构和超出分布设置中胜过先前的模型编辑方法，并克服不可能三角形。


论文地址：https://arxiv.org/pdf/2405.14758
在人类反馈强化学习（RLHF）的背景下，奖励函数通常是从人类进行的成对比较中基于随机效用模型的最大似然估计中导出的。

学习奖励函数的问题是偏好聚合的问题，我们认为这在很大程度上属于社会选择理论的范畴。

从这个角度来看，我们可以通过已建立的公理评估不同的聚合方法，检查这些方法是否符合或不符合众所周知的标准。

我们证明布拉德利-特里-卢斯（Bradley-Terry-Luce）模型及其广义推广都未能满足基本公理。

作为回应，我们开发了具有强公理保证的学习奖励函数的新规则。

从社会选择的角度来看，一个关键的创新是我们的问题具有线性结构，这极大地限制了可行规则的空间，并导致了一个我们称之为线性社会选择的新范式。


论文地址：https://arxiv.org/pdf/2405.14660
在上下文学习（In-Context Learning，ICL）中，通过在测试查询之前添加少量示例演示，使大型语言模型（Large Language Models，LLMs）能够适应未见过的任务。

尽管ICL具有很强的灵活性，但与零样本学习相比，ICL会带来相当大的计算和内存开销，并且容易受到示例演示的选择和顺序的影响。

在这项工作中，我们引入了隐式上下文学习（Implicit In-Context Learning，I2CL），这是一种创新的范式，通过将示例演示融入激活空间来解决传统ICL所面临的挑战。

I2CL首先从示例演示中生成一个精简的向量表示，即上下文向量。

然后在推理过程中，通过将上下文向量和查询激活的线性组合注入到模型的残差流中来集成上下文向量。

在三种模型架构上对九个真实世界任务进行的实证评估表明，I2CL实现了少样本性能，但零样本成本，并且对示例演示的变化表现出鲁棒性。

此外，I2CL促进了“任务ID”的新颖表示，增强了任务相似性检测，并实现了有效的迁移学习。

我们对I2CL进行了全面分析，深入探讨了其机制以及对ICL的更广泛影响。

源代码可在以下链接找到：[https://github.com/lzvv123456/i2cl](https://github.com/lzvv123456/i2cl)
论文地址：https://arxiv.org/pdf/2405.14629
在强化学习（RL）中，通过经验回放（experience replay）存储的经验会影响强化学习智能体的表现。

了解这些经验的影响对于各种目的都是有价值的，比如识别那些对表现不佳的强化学习智能体产生负面影响的经验。

一种估计经验影响的方法是留一法（leave-one-out，LOO）方法。

然而，这种方法通常在计算上是难以承受的。

本文提出了一种名为带有轮换丢弃（turn-over dropout）的策略迭代（policy iteration）方法（PITOD），可以高效地估计经验的影响。

我们评估了PITOD相对于LOO的准确性和效率，以及其估计经验影响的准确性。

然后，我们将PITOD应用于修正表现不佳的强化学习智能体，即我们使用PITOD来估计对强化学习智能体产生负面影响的经验，并消除这些经验的影响。

我们展示了通过PITOD进行修正后，强化学习智能体的表现得到了显著改善。


论文地址：https://arxiv.org/pdf/2405.14601
这个演示将展示研究助理（RA）工具的开发，用于协助六种主要类型的研究任务，这些任务被定义为标准化的指导模板，通过用户输入实例化，并最终应用为提示，供具有出色自然语言处理能力的 AI 工具（如 ChatGPT 和 Gemini）使用。

RA 解决的六个研究任务包括：创建公平的研究比较、构思研究主题、起草资助申请、撰写科学博客、协助初步同行评审以及制定增强的文献检索查询。

RA 依赖于生成式 AI 工具，如 ChatGPT 或 Gemini，这意味着相同的研究任务辅助可以在任何科学学科中提供。

我们通过在计算机科学、病毒学和气候科学领域分享 RA 输出来展示其多功能性，其中在使用 RA 工具辅助下的输出与执行相同研究任务的领域专家的输出相一致。

关键词：研究助理 · 知识图谱 · 开放研究知识图谱 · 大语言模型 · ChatGPT · Gemini
论文地址：https://arxiv.org/pdf/2405.14431
随着大型语言模型（LLMs）和检索增强生成（RAG）技术的发展，查询重写已被广泛纳入RAG系统，用于开放领域问答等下游任务。

许多研究尝试利用强化学习来使用小型模型而非昂贵的LLMs来改进查询重写。

然而，当前的方法需要注释（例如，标记的相关文档或下游答案）或预先设计的反馈奖励，这些方法缺乏泛化能力，并未利用专门为查询重写定制的信号。

在本文中，我们提出了RAFE，一个无需注释训练查询重写模型的框架。

通过利用公开可用的重新排序器，RAFE提供了与重写目标很好对齐的反馈。

实验结果表明，RAFE可以比基线模型获得更好的性能。


论文地址：https://arxiv.org/pdf/2405.14379
大语言模型（LLMs）生成新信息的潜力为研究和创新带来了潜在的重大变革。

这一点很具挑战性，因为确定LLM在训练过程中先前看到了什么可能很困难，使得“新颖性”难以证实。

在本文中，我们观察到LLMs能够在具有空间维度问题上执行复杂推理，这些问题它们以前不太可能直接遇到过。

虽然并非完美，但这表明了现有技术水平的LLMs现在能够达到的显著理解水平，支持LLMs能够产生重要新兴特性的观点。

特别是，我们发现Claude 3在这方面表现良好。


论文地址：https://arxiv.org/pdf/2405.14205
最近的努力直接利用大语言模型（LLMs）作为智能体模型来执行交互式规划任务已经显示出令人钦佩的成果。

然而，尽管取得了成就，它们在全局规划中仍然面临盲目的试错，而在局部规划中生成幻觉动作，这是由于它们对“真实”物理世界的理解不足。

本文中，我们模仿人类的心智世界知识模型，该模型在任务开始前提供全局先验知识，并在任务执行过程中保持局部动态知识，引入参数化世界知识模型（WKM）来促进智能体规划。

具体来说，我们引导智能体模型从专家和采样轨迹中自我合成知识。

然后，我们开发了WKM，提供先验任务知识以指导全局规划，并提供动态状态知识以辅助局部规划。

在三个复杂的真实世界模拟数据集上，使用三种最先进的开源LLMs，Mistral-7B、Gemma-7B和Llama-3-8B进行实验，结果表明我们的方法相对于各种强基线方法能够取得卓越的性能。

此外，我们分析表明，我们的WKM能够有效缓解盲目试错和幻觉动作问题，为智能体对世界的理解提供了有力支持。

其他有趣的发现包括：1）我们的实例级任务知识能够更好地推广到未见任务，2）弱WKM可以指导强智能体模型的规划，3）统一的WKM训练具有进一步发展的潜力。

……试错正确路径第一步……正确路径第一步世界知识模型状态知识任务知识+[][]智能体概率知识概率智能体模型智能体模型幻觉动作（a）（b）轨迹图 1: 传统智能体规划 vs. 带有世界知识模型的智能体规划。

∗同等贡献。

†通讯作者。

3代码将在 https://github.com/zjunlp/wkm 上提供。

预印本。

正在审阅。

arXiv:2405.14205v1 [cs.cl] 2024年5月23日。


论文地址：https://arxiv.org/pdf/2405.13867
大语言模型（LLMs）的规模定律为如何训练规模更大的模型以获得可预测的性能提升提供了有用的指导。

时间序列预测与语言具有类似的顺序结构，并且适合于大规模 Transformer 架构。

在这里，我们展示了基础的仅解码器时间序列 Transformer 模型表现出类似于LLMs的规模行为，而架构细节（纵横比和头数）在广泛范围内的影响很小。

我们收集了一个包含各种时间序列数据的大语料库进行训练，并首次建立了关于参数数量、数据集大小和训练计算的幂律规模关系，跨越了五个数量级。


论文地址：https://arxiv.org/pdf/2405.13568
随着国家漏洞数据库（NVD）中新漏洞数量的急剧增加，NVD 分析员将通用平台枚举（CPE）与通用漏洞和暴露（CVE）摘要相关联的工作量变得越来越繁重和缓慢。

这种延迟导致依赖 NVD 进行漏洞管理和安全评估的组织更容易受到零日攻击的威胁。

因此，提出一种技术和工具来准确快速地提取 CVE 摘要中的 CPE 是至关重要的。

在这项工作中，我们提出了 CPE 标识系统，这是一个自动化的 CPE 注释和提取系统，用于从 CVE 摘要中提取 CPE。

该系统可用作从新的 CVE 文本输入中识别 CPE 实体的工具。

此外，我们还利用深度学习模型自动化数据生成和标记过程。

由于 CVE 文本的复杂性，新的技术术语经常出现。

为了识别未来 CVE 文本中的新词，我们应用自然语言处理（NLP）命名实体识别（NER）来识别文本中的新技术行话。

我们提出的模型实现了 95.48% 的 F1 分数，99.13% 的准确率，94.83% 的精确度和 96.14% 的召回率。

我们展示了它在所有指标上比以往的自动化 CVE-CPE 标记工作表现更好，提高了超过 9%。


论文地址：https://arxiv.org/pdf/2405.13565
现代代码审查是指在代码作者提交增量代码贡献到版本控制系统之前，由一个或多个同行进行审查的过程。

现代代码审查的一个重要元素是验证代码贡献是否符合最佳实践。

虽然一些最佳实践可以通过自动验证来实现，但其他一些最佳实践通常需要人工审阅来验证。

本文报告了自动评论者（autocommenter）的开发、部署和评估，该系统由一个大型语言模型支持，可以自动学习和执行编码最佳实践。

我们为四种编程语言（C++、Java、Python 和 Go）实现了自动评论者，并在一个大型工业环境中评估了其性能和采用情况。

我们的评估表明，建立一个端到端系统来学习和执行编码最佳实践是可行的，并对开发人员的工作流程产生了积极影响。

此外，本文还报告了将这样一个系统部署到数万开发人员所面临的挑战以及相应的经验教训。

CCS 概念 •软件及其工程 →软件验证和验证。

∗本工作在 Google 完成。

未经许可，允许个人或课堂使用本作品的全部或部分数字或纸质副本，前提是不得为盈利或商业优势而制作或分发副本，并且副本必须携带本通知和第一页的完整引用。

必须尊重本作品第三方组件的版权。

对于其他所有用途，请联系所有者/作者。

AIWARE ’24，2024 年 7 月 15-16 日，巴西加林亚斯港 ©2024 所有权/作者所有。

ACM ISBN 979-8-4007-0685-1/24/07 https://doi.org/10.1145/3664646.3665664关键词 人工智能，代码审查，编码最佳实践 ACM 参考格式：Manushree Vijayvergiya，Małgorzata Salawa，Ivan Budiselić，Dan Zheng，Pascal Lamblin，Marko Ivanković，Juanjo Carin，Mateusz Lewko，Jovan Andonov，Goran Petrović，Daniel Tarlow，Petros Maniatis 和 René Just。

2024 年。

现代代码审查中编码实践的 AI 辅助评估。

在第 1 届 ACM 国际人工智能软件会议论文集（AIWARE ’24）中，2024 年 7 月 15-16 日，巴西加林亚斯港。

ACM，美国纽约，纽约，9 页。

https://doi.org/10.1145/3664646.3665664 1
论文地址：https://arxiv.org/pdf/2405.13560
对话式推荐系统 (CRS) 能够让用户用自然语言表达他们的偏好并提供反馈。

随着大语言模型 (LLMs) 的出现，利用 LLm 生成的内容增强用户与 CRS 的互动，以及增强推荐过程的潜力引起了越来越多的关注。

然而，LLM 动力的 CRS 的有效性取决于提示的使用，而对于不同的推荐领域，推荐质量的主观感知可能会有所不同。

因此，我们开发了基于 ChatGPT 的 CRS，以研究这两个因素，即提示指导 (PG) 和推荐领域 (RD)，对系统整体用户体验的影响。

我们通过在线实证研究 (n = 100) 进行了调查，采用了混合方法，对 PG 变量采用了组间设计 (有 vs. 无)，对 RD 则采用了组内设计 (书籍推荐 vs. 工作推荐)。

研究结果显示，PG 可显著提升系统的可解释性、适应性、感知易用性和透明度。

此外，用户倾向于在书籍推荐的情境下感受到更强的新奇感，并表现出更高的倾向去尝试推荐的物品，而不同于工作推荐。

此外，PG 对某些用户体验指标和互动行为的影响似乎受到推荐领域的调节，这一点在两个研究因素之间的交互作用中得到了证实。

这项工作通过研究两个显著因素，为 ChatGPT 基础的 CRS 的用户中心评估做出了贡献，并提供了实用的设计指导。

未经许可，可以为个人或课堂使用制作此工作的全部或部分数字或纸质副本，前提是不为盈利或商业优势而制作或分发副本，并且副本上必须带有此通知和第一页的完整引用。

对于此工作的组成部分的版权归其他人所有的情况，必须尊重版权。


论文地址：https://arxiv.org/pdf/2405.13290
这项研究深入探讨了元强化学习（Meta RL），重点关注定义泛化极限和确保收敛。

通过采用一种方法，本文引入了一个创新的理论框架，以精心评估元强化学习算法的有效性和性能。

我们提出了一个解释泛化极限的概念，衡量这些算法在适应学习任务的同时能够保持一致的结果。

我们的分析深入探讨了影响元强化学习适应性的因素，揭示了算法设计与任务复杂性之间的关系。

此外，我们通过证明一定条件下元强化学习策略收敛于解的条件，建立了收敛保证。

我们研究了元强化学习算法在各种情景下的收敛行为，全面理解了驱动其长期性能的因素。

这一探索涵盖了收敛和实时效率，为我们提供了对这些算法能力的视角。

关键词：元强化学习，理论分析，泛化界限，收敛保证

I. 引言
强化学习（RL）已成为一种制定决策和控制过程的方法，在从游戏到自动驾驶汽车等各种任务中取得了令人瞩目的成功。

随着RL的广泛应用，对能够高效学习并适应不同情况的算法的需求不断增长。

元强化学习（Meta RL）是朝着这个方向的一项进展。

通过学习如何学习，Meta RL旨在根据过去的经验调整任务，而无需进行大量的重新训练。

尽管具有潜力，但对Meta RL的理论理解落后于其应用。

关于这些算法如何泛化到相关任务以及在特定条件下何时达到最优解仍然存在问题。

缺乏分析使得难以预测Meta RL算法在未经测试的情况下的行为，这对于关注安全性的领域如医疗保健和航空领域至关重要。
